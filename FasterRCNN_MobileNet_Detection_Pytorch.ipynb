{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Michal287/computer_vision/blob/main/FasterRCNN_MobileNet_Detection_Pytorch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p4y-LyB1F7VE",
        "outputId": "1ea31783-57a0-4b7c-8a37-f4291eda8044"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'vision'...\n",
            "remote: Enumerating objects: 386602, done.\u001b[K\n",
            "remote: Counting objects: 100% (36842/36842), done.\u001b[K\n",
            "remote: Compressing objects: 100% (1979/1979), done.\u001b[K\n",
            "remote: Total 386602 (delta 34830), reused 36658 (delta 34746), pack-reused 349760\u001b[K\n",
            "Receiving objects: 100% (386602/386602), 765.67 MiB | 22.84 MiB/s, done.\n",
            "Resolving deltas: 100% (357356/357356), done.\n",
            "Note: switching to 'v0.9.0'.\n",
            "\n",
            "You are in 'detached HEAD' state. You can look around, make experimental\n",
            "changes and commit them, and you can discard any commits you make in this\n",
            "state without impacting any branches by switching back to a branch.\n",
            "\n",
            "If you want to create a new branch to retain commits you create, you may\n",
            "do so (now or later) by using -c with the switch command. Example:\n",
            "\n",
            "  git switch -c <new-branch-name>\n",
            "\n",
            "Or undo this operation with:\n",
            "\n",
            "  git switch -\n",
            "\n",
            "Turn off this advice by setting config variable advice.detachedHead to false\n",
            "\n",
            "HEAD is now at 01dfa8ea81 add custom user agent for download_url (#3499)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "%%shell\n",
        "\n",
        "# Download TorchVision repo to use some files from\n",
        "# references/detection\n",
        "git clone https://github.com/pytorch/vision.git\n",
        "cd vision\n",
        "git checkout v0.9.0\n",
        "\n",
        "cp references/detection/utils.py ../\n",
        "cp references/detection/transforms.py ../\n",
        "cp references/detection/coco_eval.py ../\n",
        "cp references/detection/engine.py ../\n",
        "cp references/detection/coco_utils.py ../"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y2Q8MFdviBIw",
        "outputId": "0bd8a2f0-dda0-45f3-cce1-9c125e9127c5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torch==1.11.0\n",
            "  Downloading torch-1.11.0-cp310-cp310-manylinux1_x86_64.whl (750.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m750.6/750.6 MB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch==1.11.0) (4.7.1)\n",
            "Installing collected packages: torch\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 2.0.1+cu118\n",
            "    Uninstalling torch-2.0.1+cu118:\n",
            "      Successfully uninstalled torch-2.0.1+cu118\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchaudio 2.0.2+cu118 requires torch==2.0.1, but you have torch 1.11.0 which is incompatible.\n",
            "torchdata 0.6.1 requires torch==2.0.1, but you have torch 1.11.0 which is incompatible.\n",
            "torchtext 0.15.2 requires torch==2.0.1, but you have torch 1.11.0 which is incompatible.\n",
            "torchvision 0.15.2+cu118 requires torch==2.0.1, but you have torch 1.11.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed torch-1.11.0\n",
            "Collecting torchvision==0.12.0\n",
            "  Downloading torchvision-0.12.0-cp310-cp310-manylinux1_x86_64.whl (21.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.0/21.0 MB\u001b[0m \u001b[31m20.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torchvision==0.12.0) (4.7.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision==0.12.0) (1.23.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchvision==0.12.0) (2.31.0)\n",
            "Requirement already satisfied: torch==1.11.0 in /usr/local/lib/python3.10/dist-packages (from torchvision==0.12.0) (1.11.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision==0.12.0) (9.4.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision==0.12.0) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision==0.12.0) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision==0.12.0) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision==0.12.0) (2023.7.22)\n",
            "Installing collected packages: torchvision\n",
            "  Attempting uninstall: torchvision\n",
            "    Found existing installation: torchvision 0.15.2+cu118\n",
            "    Uninstalling torchvision-0.15.2+cu118:\n",
            "      Successfully uninstalled torchvision-0.15.2+cu118\n",
            "Successfully installed torchvision-0.12.0\n"
          ]
        }
      ],
      "source": [
        "!pip install torch==1.11.0\n",
        "!pip install torchvision==0.12.0\n",
        "exit(0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PR54XftDA4pS",
        "outputId": "fa7eb0ea-99e0-4250-ad5a-6283ef91e957"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Torch version: 1.11.0+cu102, torchvision version: 0.12.0+cu102\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torchvision\n",
        "\n",
        "print(f\"Torch version: {torch.__version__}, torchvision version: {torchvision.__version__}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H3aqWpDm6B10"
      },
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i-cMqpcHvL7L"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchvision.models.detection import ssdlite320_mobilenet_v3_large\n",
        "from torchvision.transforms import ToTensor\n",
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data import DataLoader\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "from PIL import Image\n",
        "\n",
        "import os\n",
        "import random\n",
        "import json\n",
        "from google.colab import drive\n",
        "\n",
        "from pycocotools.coco import COCO\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patches as patches\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NyhnCXqP6DrU"
      },
      "source": [
        "# Check GPU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gPBObfvE6GXF",
        "outputId": "43638ce6-bf9b-46f4-8d37-c058a5eac14b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Thu Sep  7 12:36:46 2023       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 525.105.17   Driver Version: 525.105.17   CUDA Version: 12.0     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   34C    P8     9W /  70W |      0MiB / 15360MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5kW7Rm0A49wO",
        "outputId": "3d0f414f-72c7-41c6-dbff-7cdb4dc5bcee"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pytorch device: cuda\n"
          ]
        }
      ],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Pytorch device: {device}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JCI-x1VcKSKT"
      },
      "source": [
        "# Downloand data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jkhLmu09J0BU",
        "outputId": "47eba097-2d10-4182-8aac-8e72c7079651"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "\n",
            "7-Zip [64] 16.02 : Copyright (c) 1999-2016 Igor Pavlov : 2016-05-21\n",
            "p7zip Version 16.02 (locale=en_US.UTF-8,Utf16=on,HugeFiles=on,64 bits,2 CPUs Intel(R) Xeon(R) CPU @ 2.20GHz (406F0),ASM,AES-NI)\n",
            "\n",
            "Scanning the drive for archives:\n",
            "  0M Scan /content/drive/MyDrive/\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                 \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1 file, 583322523 bytes (557 MiB)\n",
            "\n",
            "Extracting archive: /content/drive/MyDrive/source.zip\n",
            "--\n",
            "Path = /content/drive/MyDrive/source.zip\n",
            "Type = zip\n",
            "Physical Size = 583322523\n",
            "\n",
            "  0%\b\b\b\b    \b\b\b\b  1% 3 - source/image_100.png\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                             \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b  2% 5 - source/image_102.png\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                             \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b  3% 8 - source/image_105.png\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                             \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b  4% 10 - source/image_107.png\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                              \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b  5% 13 - source/image_11.png\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                             \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b  6% 16 - source/image_112.png\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                              \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b  7% 19 - source/image_115.png\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                              \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 10% 24 - source/image_12.png\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                             \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 13% 31 - source/image_126.png\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                              \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 15% 36 - source/image_131.png\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                              \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 17% 41 - source/image_136.png\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                              \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 19% 47 - source/image_142.png\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                              \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 22% 53 - source/image_148.png\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                              \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 25% 60 - source/image_155.png\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                              \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 27% 66 - source/image_161.png\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                              \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 30% 73 - source/image_168.png\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                              \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 33% 79 - source/image_174.png\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                              \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 35% 84 - source/image_179.png\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                              \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 38% 90 - source/image_185.png\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                              \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 41% 98 - source/image_193.png\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                              \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 43% 104 - source/image_199.png\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                               \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 46% 110 - source/image_204.png\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                               \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 47% 112 - source/image_206.png\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                               \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 48% 114 - source/image_208.png\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                               \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 49% 118 - source/image_212.png\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                               \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 51% 121 - source/image_215.png\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                               \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 52% 125 - source/image_219.png\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                               \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 53% 127 - source/image_221.png\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                               \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 55% 130 - source/image_224.png\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                               \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 56% 132 - source/image_226.png\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                               \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 57% 135 - source/image_229.png\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                               \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 59% 139 - source/image_233.png\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                               \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 60% 143\b\b\b\b\b\b\b\b        \b\b\b\b\b\b\b\b 62% 146 - source/image_240.png\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                               \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 63% 149 - source/image_243.png\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                               \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 64% 151 - source/image_245.png\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                               \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 65% 154 - source/image_248.png\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                               \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 66% 156 - source/image_26.png\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                              \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 67% 159 - source/image_29.png\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                              \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 69% 162 - source/image_31.png\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                              \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 70% 166 - source/image_35.png\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                              \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 71% 169 - source/image_38.png\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                              \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 72% 171 - source/image_4.png\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                             \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 73% 173 - source/image_41.png\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                              \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 74% 175 - source/image_43.png\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                              \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 75% 176 - source/image_44.png\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                              \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 76% 180 - source/image_48.png\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                              \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 76% 181 - source/image_49.png\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                              \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 77% 183 - source/image_50.png\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                              \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 79% 187 - source/image_54.png\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                              \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 81% 192 - source/image_59.png\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                              \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 84% 199 - source/image_65.png\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                              \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 87% 205 - source/image_70.png\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                              \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 89% 212 - source/image_77.png\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                              \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 92% 219 - source/image_83.png\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                              \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 95% 226 - source/image_9.png\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                             \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 97% 231 - source/image_94.png\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                              \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bEverything is Ok\n",
            "\n",
            "Files: 237\n",
            "Size:       623158541\n",
            "Compressed: 583322523\n",
            "\n",
            "7-Zip [64] 16.02 : Copyright (c) 1999-2016 Igor Pavlov : 2016-05-21\n",
            "p7zip Version 16.02 (locale=en_US.UTF-8,Utf16=on,HugeFiles=on,64 bits,2 CPUs Intel(R) Xeon(R) CPU @ 2.20GHz (406F0),ASM,AES-NI)\n",
            "\n",
            "Scanning the drive for archives:\n",
            "  0M Scan /content/drive/MyDrive/\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                 \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1 file, 287681619 bytes (275 MiB)\n",
            "\n",
            "Extracting archive: /content/drive/MyDrive/source2.zip\n",
            "--\n",
            "Path = /content/drive/MyDrive/source2.zip\n",
            "Type = zip\n",
            "Physical Size = 287681619\n",
            "\n",
            "  0%\b\b\b\b    \b\b\b\b  3% 4 - source2/image_215.png\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                              \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b  7% 8 - source2/image_220.png\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                              \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 11% 12 - source2/image_225.png\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                               \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 16% 17 - source2/image_231.png\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                               \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 20% 21 - source2/image_235.png\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                               \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 21% 23 - source2/image_238.png\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                               \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 24% 25 - source2/image_240.png\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                               \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 26% 28 - source2/image_243.png\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                               \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 29% 31 - source2/image_246.png\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                               \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 33% 34 - source2/image_250.png\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                               \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 35% 37 - source2/image_253.png\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                               \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 36% 37 - source2/image_253.png\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                               \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 38% 39 - source2/image_255.png\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                               \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 40% 42\b\b\b\b\b\b\b       \b\b\b\b\b\b\b 43% 45 - source2/image_261.png\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                               \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 46% 47 - source2/image_263.png\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                               \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 49% 50 - source2/image_267.png\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                               \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 51% 53 - source2/image_270.png\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                               \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 53% 54 - source2/image_271.png\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                               \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 56% 57 - source2/image_274.png\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                               \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 60% 60 - source2/image_278.png\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                               \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 62% 62 - source2/image_281.png\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                               \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 63% 64 - source2/image_283.png\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                               \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 66% 67 - source2/image_286.png\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                               \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 70% 70 - source2/image_289.png\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                               \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 74% 74 - source2/image_293.png\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                               \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 78% 78 - source2/image_297.png\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                               \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 81% 82 - source2/image_301.png\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                               \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 85% 85 - source2/image_304.png\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                               \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 89% 89 - source2/image_308.png\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                               \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 93% 93 - source2/image_312.png\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                               \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 97% 97 - source2/image_316.png\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                               \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bEverything is Ok\n",
            "\n",
            "Files: 100\n",
            "Size:       300488081\n",
            "Compressed: 287681619\n",
            "\n",
            "7-Zip [64] 16.02 : Copyright (c) 1999-2016 Igor Pavlov : 2016-05-21\n",
            "p7zip Version 16.02 (locale=en_US.UTF-8,Utf16=on,HugeFiles=on,64 bits,2 CPUs Intel(R) Xeon(R) CPU @ 2.20GHz (406F0),ASM,AES-NI)\n",
            "\n",
            "Scanning the drive for archives:\n",
            "  0M Scan /content/drive/MyDrive/\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                 \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1 file, 110548 bytes (108 KiB)\n",
            "\n",
            "Extracting archive: /content/drive/MyDrive/annotations.zip\n",
            "--\n",
            "Path = /content/drive/MyDrive/annotations.zip\n",
            "Type = zip\n",
            "Physical Size = 110548\n",
            "\n",
            "  0%\b\b\b\b    \b\b\b\bEverything is Ok\n",
            "\n",
            "Files: 4\n",
            "Size:       856709\n",
            "Compressed: 110548\n"
          ]
        }
      ],
      "source": [
        "drive.mount('/content/drive')\n",
        "\n",
        "!7z x /content/drive/MyDrive/source.zip -odata/images\n",
        "!7z x /content/drive/MyDrive/source2.zip -odata/images\n",
        "\n",
        "!7z x /content/drive/MyDrive/annotations.zip -odata/annotations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mrdyCaWaJW_3"
      },
      "source": [
        "# Work with nested files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MenDr8cnNXIZ",
        "outputId": "30992da3-c4ba-46b2-8512-00d56be11756"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "loading annotations into memory...\n",
            "Done (t=0.02s)\n",
            "creating index...\n",
            "index created!\n",
            "loading annotations into memory...\n",
            "Done (t=0.00s)\n",
            "creating index...\n",
            "index created!\n"
          ]
        }
      ],
      "source": [
        "from pycocotools.coco import COCO\n",
        "import os\n",
        "import json\n",
        "\n",
        "def coco_prefix(coco_details):\n",
        "    for coco_detail in coco_details:\n",
        "        coco_file = coco_detail['coco_file']\n",
        "        full_path = coco_detail['prefix_path']\n",
        "\n",
        "        coco = COCO(coco_file)\n",
        "\n",
        "        # Zastąp ścieżkę obrazu podaną pełną ścieżką\n",
        "        for img_id, img_data in coco.imgs.items():\n",
        "            img_name = os.path.basename(img_data[\"file_name\"])\n",
        "            img_data[\"file_name\"] = os.path.join(full_path, img_name)\n",
        "\n",
        "        # Zapisz zmodyfikowane dane z powrotem do pliku\n",
        "        with open(coco_file, 'w') as f:\n",
        "            json.dump(coco.dataset, f)\n",
        "\n",
        "# Użyj funkcji\n",
        "coco_details = [\n",
        "    {'coco_file': '/content/data/annotations/source.json', 'prefix_path': 'source'},\n",
        "    {'coco_file': '/content/data/annotations/source2.json', 'prefix_path': 'source2'}\n",
        "]\n",
        "\n",
        "coco_prefix(coco_details)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QyqbOSH6Qmaz"
      },
      "source": [
        "# Merge files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z1YxwUPhOTMa",
        "outputId": "ed14dc6e-511b-42ca-ecf5-0f75bd31ad4a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "loading annotations into memory...\n",
            "Done (t=0.01s)\n",
            "creating index...\n",
            "index created!\n",
            "loading annotations into memory...\n",
            "Done (t=0.00s)\n",
            "creating index...\n",
            "index created!\n"
          ]
        }
      ],
      "source": [
        "from pycocotools.coco import COCO\n",
        "import os\n",
        "\n",
        "def merge_coco_files(files):\n",
        "    merged_data = {}\n",
        "    img_id_offset = 0\n",
        "    ann_id_offset = 0\n",
        "\n",
        "    for file in files:\n",
        "        coco = COCO(file)\n",
        "\n",
        "        if not merged_data:\n",
        "            merged_data = coco.dataset\n",
        "            img_id_offset += len(coco.getImgIds())\n",
        "            ann_id_offset += len(coco.getAnnIds())\n",
        "            continue\n",
        "\n",
        "        # Zmiana ID obrazów\n",
        "        for img in coco.dataset['images']:\n",
        "            img['id'] += img_id_offset\n",
        "\n",
        "        # Zmiana ID anotacji\n",
        "        for ann in coco.dataset['annotations']:\n",
        "            ann['image_id'] += img_id_offset\n",
        "            ann['id'] += ann_id_offset\n",
        "\n",
        "        # Scalenie obrazów\n",
        "        merged_data['images'].extend(coco.dataset['images'])\n",
        "\n",
        "        # Scalenie anotacji\n",
        "        merged_data['annotations'].extend(coco.dataset['annotations'])\n",
        "\n",
        "        img_id_offset += len(coco.getImgIds())\n",
        "        ann_id_offset += len(coco.getAnnIds())\n",
        "\n",
        "    return merged_data\n",
        "\n",
        "files = ['/content/data/annotations/source.json', '/content/data/annotations/source2.json']  # i tak dalej\n",
        "merged_data = merge_coco_files(files)\n",
        "\n",
        "with open('merged_coco.json', 'w') as f:\n",
        "    json.dump(merged_data, f)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w3sNJHQNQrYY"
      },
      "source": [
        "# COCO train test split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rgalg_BUQkEF",
        "outputId": "faafe638-cfaf-412b-c4b2-207b63921c81"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "loading annotations into memory...\n",
            "Done (t=0.01s)\n",
            "creating index...\n",
            "index created!\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "from pycocotools.coco import COCO\n",
        "import random\n",
        "import os\n",
        "\n",
        "def train_test_split(coco_path, output_dir, validation_split=0.2):\n",
        "    data = COCO(coco_path).dataset\n",
        "\n",
        "    # Wczytaj wszystkie identyfikatory obrazów\n",
        "    image_ids = [img_info[\"id\"] for img_info in data[\"images\"]]\n",
        "    assert len(image_ids) == len(set(image_ids)), \"There are duplicate IDs in the original data.\"\n",
        "\n",
        "    # Losowo przypisz identyfikatory obrazów do zbioru treningowego i walidacyjnego\n",
        "    num_images = len(image_ids)\n",
        "    num_validation = int(validation_split * num_images)\n",
        "    validation_ids = random.sample(image_ids, num_validation)\n",
        "    training_ids = list(set(image_ids) - set(validation_ids))\n",
        "\n",
        "    # Podziel dane na zbiory treningowy i walidacyjny\n",
        "    training_data = {\n",
        "        \"images\": [img_info for img_info in data[\"images\"] if img_info[\"id\"] in training_ids],\n",
        "        \"annotations\": [ann_info for ann_info in data[\"annotations\"] if ann_info[\"image_id\"] in training_ids],\n",
        "        \"categories\": data[\"categories\"]\n",
        "    }\n",
        "\n",
        "    validation_data = {\n",
        "        \"images\": [img_info for img_info in data[\"images\"] if img_info[\"id\"] in validation_ids],\n",
        "        \"annotations\": [ann_info for ann_info in data[\"annotations\"] if ann_info[\"image_id\"] in validation_ids],\n",
        "        \"categories\": data[\"categories\"]\n",
        "    }\n",
        "\n",
        "    # Assert to make sure there are no overlapping IDs between training and validation data\n",
        "    assert not (set(training_ids) & set(validation_ids)), \"Overlap detected between training and validation IDs.\"\n",
        "\n",
        "    # Save the data\n",
        "    for filename, output_data in [('training_data.json', training_data), ('validation_data.json', validation_data)]:\n",
        "        full_path = os.path.join(output_dir, filename)\n",
        "\n",
        "        with open(full_path, \"w\") as f:\n",
        "            json.dump(output_data, f)\n",
        "\n",
        "train_test_split('/content/merged_coco.json', '/content', validation_split=0.2)\n",
        "#train_test_split('/content/data/annotations/source.json', '/content/data/annotations', validation_split=0.2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FHZJE0Ry6Kz8"
      },
      "source": [
        "# Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bJJJ4RhF5wyt"
      },
      "outputs": [],
      "source": [
        "class CocoImageDataset(Dataset):\n",
        "    def __init__(self, coco_annotations_file, image_dir, transforms=None):\n",
        "        self.coco = coco_annotations_file\n",
        "        self.transforms = transforms\n",
        "        self.image_ids = self.coco.getImgIds()\n",
        "        self.image_dir = image_dir\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_ids)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_info = self.coco.loadImgs(self.image_ids[idx])[0]\n",
        "\n",
        "        image_path = os.path.join(self.image_dir, img_info['file_name'])\n",
        "\n",
        "        # Wczytanie obrazu z pliku\n",
        "        img = Image.open(image_path)\n",
        "\n",
        "        # Przykład: zakładamy, że pliki zdjęć są w formacie RGB\n",
        "        img = img.convert(\"RGB\")\n",
        "\n",
        "        # Pobierz informacje o bounding boxach i etykietach dla obrazu\n",
        "        ann_ids = self.coco.getAnnIds(imgIds=self.image_ids[idx])\n",
        "        annotations = self.coco.loadAnns(ann_ids)\n",
        "\n",
        "        # Pobierz etykiety klas obiektów i bounding boxy\n",
        "        labels = []\n",
        "        boxes = []\n",
        "\n",
        "        for ann in annotations:\n",
        "            bbox = ann['bbox']\n",
        "            xmin = int(bbox[0])\n",
        "            ymin = int(bbox[1])\n",
        "            xmax = int(bbox[0] + bbox[2])\n",
        "            ymax = int(bbox[1] + bbox[3])\n",
        "            boxes.append([xmin, ymin, xmax, ymax])\n",
        "\n",
        "\n",
        "        boxes = torch.as_tensor(boxes, dtype=torch.float32)\n",
        "        labels = torch.ones((len(annotations),), dtype=torch.int64)\n",
        "\n",
        "        image_id = img_info['id']\n",
        "        area = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])\n",
        "        # suppose all instances are not crowd\n",
        "        iscrowd = torch.zeros((len(annotations),), dtype=torch.int64)\n",
        "\n",
        "        if self.transforms is not None:\n",
        "            data = {\n",
        "                'image': np.array(img),          # obraz jako np.array lub PIL.Image\n",
        "                'bboxes': np.array(boxes), # lista bounding boxów (x_min, y_min, x_max, y_max)\n",
        "                'category_id': np.array(labels)  # lista etykiet klas\n",
        "            }\n",
        "\n",
        "            transformed = self.transforms(**data)\n",
        "            img = transformed['image']\n",
        "            boxes = transformed['bboxes']\n",
        "\n",
        "        target = {}\n",
        "        target[\"boxes\"] = torch.tensor([[int(x1), int(y1), int(x2), int(y2)] for x1, y1, x2, y2 in boxes]).float()\n",
        "        target[\"labels\"] = labels\n",
        "        target[\"image_id\"] = image_id\n",
        "        target[\"area\"] = area\n",
        "        target[\"iscrowd\"] = iscrowd\n",
        "\n",
        "        # if self.transforms is not None:\n",
        "        #     img, target = self.transforms(img, target)\n",
        "\n",
        "        return img, target"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mxLdnPne53yk",
        "outputId": "4598042c-cb78-4246-d946-3c337785209d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "loading annotations into memory...\n",
            "Done (t=0.03s)\n",
            "creating index...\n",
            "index created!\n",
            "loading annotations into memory...\n",
            "Done (t=0.00s)\n",
            "creating index...\n",
            "index created!\n"
          ]
        }
      ],
      "source": [
        "from torchvision import utils\n",
        "import albumentations as A\n",
        "from albumentations.pytorch import ToTensorV2\n",
        "\n",
        "def get_transform(train):\n",
        "    transforms = []\n",
        "    # Kolejność transformacji została zmieniona\n",
        "    if train:\n",
        "        transforms.append(A.HorizontalFlip(p=0.2))\n",
        "        # transforms.append(A.VerticalFlip(p=0.2))\n",
        "        transforms.append(A.Blur(blur_limit=2, p=0.25))\n",
        "        transforms.append(A.RandomBrightnessContrast(brightness_limit=0.25, contrast_limit=0.15, p=0.5))\n",
        "\n",
        "    transforms.append(A.Resize(320, 320))\n",
        "    transforms.append(A.Normalize(mean=[0, 0, 0], std=[255, 255, 255], max_pixel_value=1.0))\n",
        "    transforms.append(ToTensorV2())\n",
        "    bbox_params = A.BboxParams(format='pascal_voc', min_visibility=0.5, label_fields=['category_id'])\n",
        "    return A.Compose(transforms, bbox_params=bbox_params)\n",
        "\n",
        "\n",
        "train_dataset = CocoImageDataset(COCO('/content/training_data.json'), image_dir='/content/data/images/', transforms=get_transform(train=True))\n",
        "validation_dataset = CocoImageDataset(COCO('/content/validation_data.json'), image_dir='/content/data/images/', transforms=get_transform(train=False))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fvFpBIxb3SnF"
      },
      "outputs": [],
      "source": [
        "batch_size = 16\n",
        "\n",
        "train_dataset_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2, collate_fn=lambda x: tuple(zip(*x)))\n",
        "validation_dataset_loader = DataLoader(validation_dataset, batch_size=batch_size, num_workers=2, collate_fn=lambda x: tuple(zip(*x)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vTbm95Qk8vXk"
      },
      "source": [
        "# Check Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 425
        },
        "id": "X8Ph32uM_sHa",
        "outputId": "8aae0adb-e41a-4815-fc3a-58d3f3a7ff17"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Invalid format for data: 'image_id' should be a Tensor.\n",
            "Invalid format for data: 'image_id' should be a Tensor.\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-15-80eaf09fc775>\u001b[0m in \u001b[0;36m<cell line: 32>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;31m# Użyj funkcji do sprawdzenia danych podczas treningu\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_dataset_loader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mimage\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    528\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    529\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 530\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    531\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    532\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1205\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1206\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_shutdown\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tasks_outstanding\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1207\u001b[0;31m             \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1208\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tasks_outstanding\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1209\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_get_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1171\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1172\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1173\u001b[0;31m                 \u001b[0msuccess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_try_get_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1174\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0msuccess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1175\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_try_get_data\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1009\u001b[0m         \u001b[0;31m#   (bool: whether successfully get data, any: data if successful else None)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1010\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1011\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data_queue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1012\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1013\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/multiprocessing/queues.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    120\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_rlock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelease\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m         \u001b[0;31m# unserialize the data after having released the lock\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 122\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_ForkingPickler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mqsize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/multiprocessing/reductions.py\u001b[0m in \u001b[0;36mrebuild_storage_fd\u001b[0;34m(cls, df, size)\u001b[0m\n\u001b[1;32m    293\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mrebuild_storage_fd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 295\u001b[0;31m     \u001b[0mfd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    296\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m         \u001b[0mstorage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstorage_from_cache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfd_id\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/multiprocessing/resource_sharer.py\u001b[0m in \u001b[0;36mdetach\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     55\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m             \u001b[0;34m'''Get the fd.  This should only be called once.'''\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m             \u001b[0;32mwith\u001b[0m \u001b[0m_resource_sharer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_connection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_id\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mconn\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_handle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/multiprocessing/resource_sharer.py\u001b[0m in \u001b[0;36mget_connection\u001b[0;34m(ident)\u001b[0m\n\u001b[1;32m     84\u001b[0m         \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mconnection\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mClient\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m         \u001b[0maddress\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mident\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m         \u001b[0mc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mClient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maddress\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mauthkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcurrent_process\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauthkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     87\u001b[0m         \u001b[0mc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetpid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/multiprocessing/connection.py\u001b[0m in \u001b[0;36mClient\u001b[0;34m(address, family, authkey)\u001b[0m\n\u001b[1;32m    506\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    507\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mauthkey\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 508\u001b[0;31m         \u001b[0manswer_challenge\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mauthkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    509\u001b[0m         \u001b[0mdeliver_challenge\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mauthkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    510\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/multiprocessing/connection.py\u001b[0m in \u001b[0;36manswer_challenge\u001b[0;34m(connection, authkey)\u001b[0m\n\u001b[1;32m    750\u001b[0m         raise ValueError(\n\u001b[1;32m    751\u001b[0m             \"Authkey must be bytes, not {0!s}\".format(type(authkey)))\n\u001b[0;32m--> 752\u001b[0;31m     \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconnection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_bytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m256\u001b[0m\u001b[0;34m)\u001b[0m         \u001b[0;31m# reject large message\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    753\u001b[0m     \u001b[0;32massert\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCHALLENGE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mCHALLENGE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'message = %r'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    754\u001b[0m     \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCHALLENGE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/multiprocessing/connection.py\u001b[0m in \u001b[0;36mrecv_bytes\u001b[0;34m(self, maxlength)\u001b[0m\n\u001b[1;32m    214\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmaxlength\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mmaxlength\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"negative maxlength\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m         \u001b[0mbuf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_recv_bytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmaxlength\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbuf\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_bad_message_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/multiprocessing/connection.py\u001b[0m in \u001b[0;36m_recv_bytes\u001b[0;34m(self, maxsize)\u001b[0m\n\u001b[1;32m    412\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    413\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_recv_bytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaxsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 414\u001b[0;31m         \u001b[0mbuf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_recv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    415\u001b[0m         \u001b[0msize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstruct\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munpack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"!i\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetvalue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    416\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0msize\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/multiprocessing/connection.py\u001b[0m in \u001b[0;36m_recv\u001b[0;34m(self, size, read)\u001b[0m\n\u001b[1;32m    377\u001b[0m         \u001b[0mremaining\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    378\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0mremaining\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 379\u001b[0;31m             \u001b[0mchunk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mremaining\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    380\u001b[0m             \u001b[0mn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    381\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mn\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "def validate_data(images, targets):\n",
        "    # walidacja obrazów\n",
        "    if not isinstance(images, torch.Tensor):\n",
        "        raise ValueError(\"Images should be of type Tensor.\")\n",
        "\n",
        "    if not (images.dim() == 3 and images.size(0) == 3):\n",
        "        raise ValueError(\"Images should have dimensions [3, H, W].\")\n",
        "\n",
        "    # walidacja targetów\n",
        "    required_keys = ['boxes', 'labels']\n",
        "    optional_keys = ['image_id', 'area', 'iscrowd']\n",
        "\n",
        "    for t in targets:\n",
        "        # sprawdź czy wszystkie wymagane klucze są obecne\n",
        "        if not all(key in t for key in required_keys):\n",
        "            raise ValueError(f\"Target is missing one of the required keys {required_keys}\")\n",
        "\n",
        "        # sprawdź czy 'boxes' i 'labels' mają poprawny format\n",
        "        if not (isinstance(t['boxes'], torch.Tensor) and t['boxes'].dim() == 2 and t['boxes'].size(1) == 4):\n",
        "            raise ValueError(\"'boxes' should be a 2D Tensor with shape (_, 4).\")\n",
        "\n",
        "        if not (isinstance(t['labels'], torch.Tensor) and t['labels'].dim() == 1):\n",
        "            raise ValueError(\"'labels' should be a 1D Tensor.\")\n",
        "\n",
        "        # jeśli istnieją, sprawdź poprawność opcjonalnych kluczy\n",
        "        for key in optional_keys:\n",
        "            if key in t:\n",
        "                if not isinstance(t[key], torch.Tensor):\n",
        "                    raise ValueError(f\"'{key}' should be a Tensor.\")\n",
        "\n",
        "# Użyj funkcji do sprawdzenia danych podczas treningu\n",
        "for images, targets in train_dataset_loader:\n",
        "    try:\n",
        "        for image in images:\n",
        "            validate_data(image, targets)\n",
        "            break\n",
        "    except ValueError as e:\n",
        "        print(f\"Invalid format for data: {e}\")\n",
        "        continue"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LpUQXP6XGNkr"
      },
      "source": [
        "# Show data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sPhuAzhgUB0w"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patches as patches\n",
        "import numpy as np\n",
        "\n",
        "def show_images(dataloader, num_images, figsize=(15, 15)):\n",
        "    # Pobieramy batch z dataloader\n",
        "    images, targets = next(iter(dataloader))\n",
        "\n",
        "    # Wybieramy pierwsze `num_images` obrazów i targety\n",
        "    images = images[:num_images]\n",
        "    targets = targets[:num_images]\n",
        "\n",
        "    # Obliczamy liczbę rzędów potrzebnych do wyświetlenia obrazów w 3 kolumnach\n",
        "    num_rows = int(np.ceil(num_images / 3))\n",
        "\n",
        "    fig = plt.figure(figsize=figsize)\n",
        "\n",
        "    for i in range(num_images):\n",
        "        ax = fig.add_subplot(num_rows, 3, i+1)\n",
        "\n",
        "        # Przekształcamy obraz z tensora do formatu PIL i następnie do ndarray dla wyświetlenia w matplotlib\n",
        "        img = transforms.ToPILImage()(images[i]).convert(\"RGB\")\n",
        "        img = np.array(img)\n",
        "\n",
        "        # Rysujemy obraz\n",
        "        ax.imshow(img)\n",
        "\n",
        "        # Rysujemy bounding boxy\n",
        "        for box in targets[i]['boxes']:\n",
        "            rect = patches.Rectangle((box[0], box[1]), box[2]-box[0], box[3]-box[1], linewidth=2, edgecolor='b', facecolor='none')\n",
        "            ax.add_patch(rect)\n",
        "\n",
        "        # Wyłączamy wyświetlanie osi\n",
        "        plt.axis('off')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "show_images(train_dataset_loader, 9)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vZywN5R86MgU"
      },
      "source": [
        "# Load Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "_MpZHDZUp9o9"
      },
      "outputs": [],
      "source": [
        "# Change in coco_utils [132] targets[\"image_id\"].items() to targets[\"image_id\"]\n",
        "from torchvision.models.detection import fasterrcnn_mobilenet_v3_large_fpn\n",
        "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
        "import utils\n",
        "from torchsummary import summary\n",
        "\n",
        "\n",
        "model = fasterrcnn_mobilenet_v3_large_fpn(pretrained=True)\n",
        "in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
        "model.roi_heads.box_predictor = FastRCNNPredictor(in_features, 2)\n",
        "\n",
        "# Inicjalizuj wagi dla nowej warstwy klasyfikacyjnej\n",
        "from torch.nn.init import xavier_uniform_\n",
        "xavier_uniform_(model.roi_heads.box_predictor.cls_score.weight)\n",
        "torch.nn.init.constant_(model.roi_heads.box_predictor.cls_score.bias, 0)\n",
        "\n",
        "model.train()\n",
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "model.to(device)\n",
        "\n",
        "# move model to the right device\n",
        "model.to(device)\n",
        "\n",
        "# construct an optimizer\n",
        "params = [p for p in model.parameters() if p.requires_grad]\n",
        "optimizer = torch.optim.SGD(params, lr=0.01,\n",
        "                            momentum=0.937, weight_decay=0.0005)\n",
        "# and a learning rate scheduler\n",
        "lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer,\n",
        "                                                step_size=3,\n",
        "                                                gamma=0.1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iq0lQCA_5vHH"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "import sys\n",
        "import time\n",
        "import torch\n",
        "\n",
        "import torchvision.models.detection.mask_rcnn\n",
        "\n",
        "from coco_utils import get_coco_api_from_dataset\n",
        "from coco_eval import CocoEvaluator\n",
        "import utils\n",
        "\n",
        "\n",
        "def train_one_epoch(model, optimizer, data_loader, device, epoch, print_freq, scaler=None):\n",
        "    model.train()\n",
        "    metric_logger = utils.MetricLogger(delimiter=\"  \")\n",
        "    metric_logger.add_meter(\"lr\", utils.SmoothedValue(window_size=1, fmt=\"{value:.6f}\"))\n",
        "    header = f\"Epoch: [{epoch}]\"\n",
        "\n",
        "    lr_scheduler = None\n",
        "    if epoch == 0:\n",
        "        warmup_factor = 1.0 / 1000\n",
        "        warmup_iters = min(1000, len(data_loader) - 1)\n",
        "\n",
        "        lr_scheduler = torch.optim.lr_scheduler.LinearLR(\n",
        "            optimizer, start_factor=warmup_factor, total_iters=warmup_iters\n",
        "        )\n",
        "\n",
        "    for images, targets in metric_logger.log_every(data_loader, print_freq, header):\n",
        "        images = list(image.to(device) for image in images)\n",
        "        targets = [{k: v.to(device) if isinstance(v, torch.Tensor) else v for k, v in t.items()} for t in targets]\n",
        "\n",
        "        loss_dict = model(images, targets)\n",
        "\n",
        "        losses = sum(loss for loss in loss_dict.values())\n",
        "\n",
        "        # reduce losses over all GPUs for logging purposes\n",
        "        loss_dict_reduced = utils.reduce_dict(loss_dict)\n",
        "        losses_reduced = sum(loss for loss in loss_dict_reduced.values())\n",
        "\n",
        "        loss_value = losses_reduced.item()\n",
        "\n",
        "        if not math.isfinite(loss_value):\n",
        "            print(\"Loss is {}, stopping training\".format(loss_value))\n",
        "            print(loss_dict_reduced)\n",
        "            sys.exit(1)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        losses.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if lr_scheduler is not None:\n",
        "            lr_scheduler.step()\n",
        "\n",
        "        metric_logger.update(loss=losses_reduced, **loss_dict_reduced)\n",
        "        metric_logger.update(lr=optimizer.param_groups[0][\"lr\"])\n",
        "\n",
        "    return metric_logger\n",
        "\n",
        "\n",
        "def _get_iou_types(model):\n",
        "    model_without_ddp = model\n",
        "    if isinstance(model, torch.nn.parallel.DistributedDataParallel):\n",
        "        model_without_ddp = model.module\n",
        "    iou_types = [\"bbox\"]\n",
        "    if isinstance(model_without_ddp, torchvision.models.detection.MaskRCNN):\n",
        "        iou_types.append(\"segm\")\n",
        "    if isinstance(model_without_ddp, torchvision.models.detection.KeypointRCNN):\n",
        "        iou_types.append(\"keypoints\")\n",
        "    return iou_types\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def evaluate(model, data_loader, device):\n",
        "    n_threads = torch.get_num_threads()\n",
        "    # FIXME remove this and make paste_masks_in_image run on the GPU\n",
        "    torch.set_num_threads(1)\n",
        "    cpu_device = torch.device(\"cpu\")\n",
        "    model.eval()\n",
        "    metric_logger = utils.MetricLogger(delimiter=\"  \")\n",
        "    header = 'Test:'\n",
        "\n",
        "    coco = get_coco_api_from_dataset(data_loader.dataset)\n",
        "    iou_types = _get_iou_types(model)\n",
        "    coco_evaluator = CocoEvaluator(coco, iou_types)\n",
        "\n",
        "    for images, targets in metric_logger.log_every(data_loader, 100, header):\n",
        "        images = list(img.to(device) for img in images)\n",
        "\n",
        "        torch.cuda.synchronize()\n",
        "        model_time = time.time()\n",
        "        outputs = model(images)\n",
        "\n",
        "        outputs = [{k: v.to(cpu_device) for k, v in t.items()} for t in outputs]\n",
        "        model_time = time.time() - model_time\n",
        "\n",
        "        res = {target[\"image_id\"]: output for target, output in zip(targets, outputs)}\n",
        "        evaluator_time = time.time()\n",
        "        coco_evaluator.update(res)\n",
        "        evaluator_time = time.time() - evaluator_time\n",
        "        metric_logger.update(model_time=model_time, evaluator_time=evaluator_time)\n",
        "\n",
        "    # gather the stats from all processes\n",
        "    metric_logger.synchronize_between_processes()\n",
        "    print(\"Averaged stats:\", metric_logger)\n",
        "    coco_evaluator.synchronize_between_processes()\n",
        "\n",
        "    # accumulate predictions from all images\n",
        "    coco_evaluator.accumulate()\n",
        "    coco_evaluator.summarize()\n",
        "    torch.set_num_threads(n_threads)\n",
        "    return coco_evaluator\n",
        "\n",
        "def convert_to_coco_api(ds):\n",
        "    coco_ds = COCO()\n",
        "    # annotation IDs need to start at 1, not 0, see torchvision issue #1530\n",
        "    ann_id = 1\n",
        "    dataset = {'images': [], 'categories': [], 'annotations': []}\n",
        "    categories = set()\n",
        "    for img_idx in range(len(ds)):\n",
        "        # find better way to get target\n",
        "        # targets = ds.get_annotations(img_idx)\n",
        "        img, targets = ds[img_idx]\n",
        "        image_id = targets[\"image_id\"]\n",
        "        img_dict = {}\n",
        "        img_dict['id'] = image_id\n",
        "        img_dict['height'] = img.shape[-2]\n",
        "        img_dict['width'] = img.shape[-1]\n",
        "        dataset['images'].append(img_dict)\n",
        "        bboxes = targets[\"boxes\"]\n",
        "        bboxes[:, 2:] -= bboxes[:, :2]\n",
        "        bboxes = bboxes.tolist()\n",
        "        labels = targets['labels'].tolist()\n",
        "        areas = targets['area'].tolist()\n",
        "        iscrowd = targets['iscrowd'].tolist()\n",
        "        if 'masks' in targets:\n",
        "            masks = targets['masks']\n",
        "            # make masks Fortran contiguous for coco_mask\n",
        "            masks = masks.permute(0, 2, 1).contiguous().permute(0, 2, 1)\n",
        "        if 'keypoints' in targets:\n",
        "            keypoints = targets['keypoints']\n",
        "            keypoints = keypoints.reshape(keypoints.shape[0], -1).tolist()\n",
        "        num_objs = len(bboxes)\n",
        "        for i in range(num_objs):\n",
        "            ann = {}\n",
        "            ann['image_id'] = image_id\n",
        "            ann['bbox'] = bboxes[i]\n",
        "            ann['category_id'] = labels[i]\n",
        "            categories.add(labels[i])\n",
        "            ann['area'] = areas[i]\n",
        "            ann['iscrowd'] = iscrowd[i]\n",
        "            ann['id'] = ann_id\n",
        "            if 'masks' in targets:\n",
        "                ann[\"segmentation\"] = coco_mask.encode(masks[i].numpy())\n",
        "            if 'keypoints' in targets:\n",
        "                ann['keypoints'] = keypoints[i]\n",
        "                ann['num_keypoints'] = sum(k != 0 for k in keypoints[i][2::3])\n",
        "            dataset['annotations'].append(ann)\n",
        "            ann_id += 1\n",
        "    dataset['categories'] = [{'id': i} for i in sorted(categories)]\n",
        "    coco_ds.dataset = dataset\n",
        "    coco_ds.createIndex()\n",
        "    return coco_ds\n",
        "\n",
        "\n",
        "def get_coco_api_from_dataset(dataset):\n",
        "    for _ in range(10):\n",
        "        if isinstance(dataset, torchvision.datasets.CocoDetection):\n",
        "            break\n",
        "        if isinstance(dataset, torch.utils.data.Subset):\n",
        "            dataset = dataset.dataset\n",
        "    if isinstance(dataset, torchvision.datasets.CocoDetection):\n",
        "        return dataset.coco\n",
        "    return convert_to_coco_api(dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nasvVt4Bml0m"
      },
      "source": [
        "# Train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zHsey8dUmlTp"
      },
      "outputs": [],
      "source": [
        "# let's train it for 10 epochs\n",
        "num_epochs = 15\n",
        "epoch_train_history = []\n",
        "epoch_val_history = []\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    epoch_train_loss = train_one_epoch(model, optimizer, train_dataset_loader, device, epoch, print_freq=10)\n",
        "    # update the learning rate\n",
        "    lr_scheduler.step()\n",
        "\n",
        "    epoch_train_evaluate = evaluate(model, train_dataset_loader, device=device)\n",
        "    epoch_train_history.append(epoch_train_evaluate)\n",
        "\n",
        "    epoch_val_evaluate = evaluate(model, validation_dataset_loader, device=device)\n",
        "    epoch_val_history.append(epoch_val_evaluate)\n",
        "print(\"That's it!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5yntgxMkmpUB"
      },
      "source": [
        "# Plot functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G631uX2qqTJS"
      },
      "outputs": [],
      "source": [
        "def get_results(history):\n",
        "    results_labels = ['Average Precision IoU=0.50:0.95 area=all',\n",
        "                  'Average Precision IoU=0.50 area=all',\n",
        "                  'Average Precision IoU=0.75 area=all',\n",
        "                  'Average Precision IoU=0.50:0.95 area=small',\n",
        "                  'Average Precision IoU=0.50:0.95 area=medium',\n",
        "                  'Average Precision IoU=0.50:0.95 area=large',\n",
        "                  'Average Recall IoU=0.50:0.95 area=all',\n",
        "                  'Average Recall IoU=0.50 area=all',\n",
        "                  'Average Recall IoU=0.75 area=all',\n",
        "                  'Average Recall IoU=0.50:0.95 area=small',\n",
        "                  'Average Recall IoU=0.50:0.95 area=medium',\n",
        "                  'Average Recall IoU=0.50:0.95 area=large']\n",
        "\n",
        "    dict_results_precision = {key: [] for key in results_labels[:6]}\n",
        "    dict_results_recall = {key: [] for key in results_labels[6:]}\n",
        "\n",
        "    for results in history:\n",
        "        scores = results.coco_eval['bbox'].stats\n",
        "\n",
        "        for idx, label in enumerate(results_labels):\n",
        "            if idx < 6:\n",
        "                dict_results_precision[label].append(scores[idx])\n",
        "            else:\n",
        "                dict_results_recall[label].append(scores[idx])\n",
        "\n",
        "    return dict_results_precision, dict_results_recall\n",
        "\n",
        "train_results_precision, train_results_recall = get_results(epoch_train_history)\n",
        "val_results_precision, val_results_recall = get_results(epoch_val_history)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LqubcaMkiNq_"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plot_in_grid(train_results, val_results, rows, cols):\n",
        "    if len(train_results) != len(val_results):\n",
        "        print(\"Błąd: listy train_results i val_results muszą mieć tę samą długość.\")\n",
        "        return\n",
        "\n",
        "    if len(train_results) > rows * cols:\n",
        "        print(\"Błąd: liczba kombinacji train_results i val_results jest większa niż liczba dostępnych miejsc w siatce.\")\n",
        "        return\n",
        "\n",
        "    fig, axes = plt.subplots(rows, cols, figsize=(5*cols,5*rows))\n",
        "\n",
        "    if rows == 1:\n",
        "        axes = [axes]\n",
        "    if cols == 1:\n",
        "        axes = [[ax] for ax in axes]\n",
        "\n",
        "    for idx, (train_label, val_label) in enumerate(zip(train_results, val_results)):\n",
        "        row = idx // cols\n",
        "        col = idx % cols\n",
        "        ax = axes[row][col]\n",
        "        epochs = range(1, len(train_results[train_label]) + 1)\n",
        "\n",
        "        # Rysuj stratę dla danych treningowych:\n",
        "        ax.plot(epochs, train_results[train_label], 'r', label='Train')\n",
        "\n",
        "        # Rysuj stratę dla danych walidacyjnych:\n",
        "        ax.plot(epochs, val_results[val_label], 'b', label='Validation')\n",
        "\n",
        "        # Dodaj tytuł i etykiety:\n",
        "        ax.set_title(train_label)\n",
        "        ax.set_xlabel('Epochs')\n",
        "        ax.set_ylabel('Loss')\n",
        "        ax.legend()\n",
        "\n",
        "    # Ukrywanie pustych subwykresów:\n",
        "    for idx in range(len(train_results), rows*cols):\n",
        "        row = idx // cols\n",
        "        col = idx % cols\n",
        "        axes[row][col].axis('off')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TQ1I6U8ZmbuR"
      },
      "source": [
        "# Precison"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L7chrX7rmYAw"
      },
      "outputs": [],
      "source": [
        "plot_in_grid(train_results_precision, val_results_precision, 2, 3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0p8VhCRBmeLE"
      },
      "source": [
        "# Recall"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O_nVuYjLmZWe"
      },
      "outputs": [],
      "source": [
        "plot_in_grid(train_results_recall, val_results_recall, 2, 3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        },
        "id": "u9exB-M9h1IR",
        "outputId": "f9875197-4787-46ae-c1bd-6a1bcd860fe4"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Przykładowe dane:\n",
        "train_losses = [1.5, 1.2, 0.9, 0.7, 0.5]\n",
        "val_losses = [1.4, 1.3, 1.0, 0.8, 0.6]\n",
        "\n",
        "# Utwórz zakres dla epok. Zakładając, że liczba epok to długość jednej z list strat:\n",
        "epochs = range(1, len(train_losses) + 1)\n",
        "\n",
        "# Rysuj stratę dla danych treningowych:\n",
        "plt.plot(epochs, train_losses, 'r', label='Train')\n",
        "\n",
        "# Rysuj stratę dla danych walidacyjnych:\n",
        "plt.plot(epochs, val_losses, 'b', label='Validation')\n",
        "\n",
        "# Dodaj tytuł i etykiety:\n",
        "plt.title('Training and Validation Loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "\n",
        "# Wyświetl wykres:\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GXbrv2TStjFr"
      },
      "outputs": [],
      "source": [
        "torch.save(model.state_dict(), f'/content/faster_rcnn_mobilenetv2_50.pth')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iBclu_aQGKAl",
        "outputId": "f0c6aebf-5feb-445c-981c-1c0ad5c629a9"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "FasterRCNN(\n",
              "  (transform): GeneralizedRCNNTransform(\n",
              "      Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
              "      Resize(min_size=(320,), max_size=640, mode='bilinear')\n",
              "  )\n",
              "  (backbone): BackboneWithFPN(\n",
              "    (body): IntermediateLayerGetter(\n",
              "      (0): Conv2dNormActivation(\n",
              "        (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "        (1): FrozenBatchNorm2d(16, eps=1e-05)\n",
              "        (2): Hardswish()\n",
              "      )\n",
              "      (1): InvertedResidual(\n",
              "        (block): Sequential(\n",
              "          (0): Conv2dNormActivation(\n",
              "            (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=16, bias=False)\n",
              "            (1): FrozenBatchNorm2d(16, eps=1e-05)\n",
              "            (2): ReLU(inplace=True)\n",
              "          )\n",
              "          (1): Conv2dNormActivation(\n",
              "            (0): Conv2d(16, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "            (1): FrozenBatchNorm2d(16, eps=1e-05)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (2): InvertedResidual(\n",
              "        (block): Sequential(\n",
              "          (0): Conv2dNormActivation(\n",
              "            (0): Conv2d(16, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "            (1): FrozenBatchNorm2d(64, eps=1e-05)\n",
              "            (2): ReLU(inplace=True)\n",
              "          )\n",
              "          (1): Conv2dNormActivation(\n",
              "            (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=64, bias=False)\n",
              "            (1): FrozenBatchNorm2d(64, eps=1e-05)\n",
              "            (2): ReLU(inplace=True)\n",
              "          )\n",
              "          (2): Conv2dNormActivation(\n",
              "            (0): Conv2d(64, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "            (1): FrozenBatchNorm2d(24, eps=1e-05)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (3): InvertedResidual(\n",
              "        (block): Sequential(\n",
              "          (0): Conv2dNormActivation(\n",
              "            (0): Conv2d(24, 72, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "            (1): FrozenBatchNorm2d(72, eps=1e-05)\n",
              "            (2): ReLU(inplace=True)\n",
              "          )\n",
              "          (1): Conv2dNormActivation(\n",
              "            (0): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=72, bias=False)\n",
              "            (1): FrozenBatchNorm2d(72, eps=1e-05)\n",
              "            (2): ReLU(inplace=True)\n",
              "          )\n",
              "          (2): Conv2dNormActivation(\n",
              "            (0): Conv2d(72, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "            (1): FrozenBatchNorm2d(24, eps=1e-05)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (4): InvertedResidual(\n",
              "        (block): Sequential(\n",
              "          (0): Conv2dNormActivation(\n",
              "            (0): Conv2d(24, 72, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "            (1): FrozenBatchNorm2d(72, eps=1e-05)\n",
              "            (2): ReLU(inplace=True)\n",
              "          )\n",
              "          (1): Conv2dNormActivation(\n",
              "            (0): Conv2d(72, 72, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=72, bias=False)\n",
              "            (1): FrozenBatchNorm2d(72, eps=1e-05)\n",
              "            (2): ReLU(inplace=True)\n",
              "          )\n",
              "          (2): SqueezeExcitation(\n",
              "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
              "            (fc1): Conv2d(72, 24, kernel_size=(1, 1), stride=(1, 1))\n",
              "            (fc2): Conv2d(24, 72, kernel_size=(1, 1), stride=(1, 1))\n",
              "            (activation): ReLU()\n",
              "            (scale_activation): Hardsigmoid()\n",
              "          )\n",
              "          (3): Conv2dNormActivation(\n",
              "            (0): Conv2d(72, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "            (1): FrozenBatchNorm2d(40, eps=1e-05)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (5): InvertedResidual(\n",
              "        (block): Sequential(\n",
              "          (0): Conv2dNormActivation(\n",
              "            (0): Conv2d(40, 120, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "            (1): FrozenBatchNorm2d(120, eps=1e-05)\n",
              "            (2): ReLU(inplace=True)\n",
              "          )\n",
              "          (1): Conv2dNormActivation(\n",
              "            (0): Conv2d(120, 120, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=120, bias=False)\n",
              "            (1): FrozenBatchNorm2d(120, eps=1e-05)\n",
              "            (2): ReLU(inplace=True)\n",
              "          )\n",
              "          (2): SqueezeExcitation(\n",
              "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
              "            (fc1): Conv2d(120, 32, kernel_size=(1, 1), stride=(1, 1))\n",
              "            (fc2): Conv2d(32, 120, kernel_size=(1, 1), stride=(1, 1))\n",
              "            (activation): ReLU()\n",
              "            (scale_activation): Hardsigmoid()\n",
              "          )\n",
              "          (3): Conv2dNormActivation(\n",
              "            (0): Conv2d(120, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "            (1): FrozenBatchNorm2d(40, eps=1e-05)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (6): InvertedResidual(\n",
              "        (block): Sequential(\n",
              "          (0): Conv2dNormActivation(\n",
              "            (0): Conv2d(40, 120, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "            (1): FrozenBatchNorm2d(120, eps=1e-05)\n",
              "            (2): ReLU(inplace=True)\n",
              "          )\n",
              "          (1): Conv2dNormActivation(\n",
              "            (0): Conv2d(120, 120, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=120, bias=False)\n",
              "            (1): FrozenBatchNorm2d(120, eps=1e-05)\n",
              "            (2): ReLU(inplace=True)\n",
              "          )\n",
              "          (2): SqueezeExcitation(\n",
              "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
              "            (fc1): Conv2d(120, 32, kernel_size=(1, 1), stride=(1, 1))\n",
              "            (fc2): Conv2d(32, 120, kernel_size=(1, 1), stride=(1, 1))\n",
              "            (activation): ReLU()\n",
              "            (scale_activation): Hardsigmoid()\n",
              "          )\n",
              "          (3): Conv2dNormActivation(\n",
              "            (0): Conv2d(120, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "            (1): FrozenBatchNorm2d(40, eps=1e-05)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (7): InvertedResidual(\n",
              "        (block): Sequential(\n",
              "          (0): Conv2dNormActivation(\n",
              "            (0): Conv2d(40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "            (1): FrozenBatchNorm2d(240, eps=1e-05)\n",
              "            (2): Hardswish()\n",
              "          )\n",
              "          (1): Conv2dNormActivation(\n",
              "            (0): Conv2d(240, 240, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=240, bias=False)\n",
              "            (1): FrozenBatchNorm2d(240, eps=1e-05)\n",
              "            (2): Hardswish()\n",
              "          )\n",
              "          (2): Conv2dNormActivation(\n",
              "            (0): Conv2d(240, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "            (1): FrozenBatchNorm2d(80, eps=1e-05)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (8): InvertedResidual(\n",
              "        (block): Sequential(\n",
              "          (0): Conv2dNormActivation(\n",
              "            (0): Conv2d(80, 200, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "            (1): FrozenBatchNorm2d(200, eps=1e-05)\n",
              "            (2): Hardswish()\n",
              "          )\n",
              "          (1): Conv2dNormActivation(\n",
              "            (0): Conv2d(200, 200, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=200, bias=False)\n",
              "            (1): FrozenBatchNorm2d(200, eps=1e-05)\n",
              "            (2): Hardswish()\n",
              "          )\n",
              "          (2): Conv2dNormActivation(\n",
              "            (0): Conv2d(200, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "            (1): FrozenBatchNorm2d(80, eps=1e-05)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (9): InvertedResidual(\n",
              "        (block): Sequential(\n",
              "          (0): Conv2dNormActivation(\n",
              "            (0): Conv2d(80, 184, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "            (1): FrozenBatchNorm2d(184, eps=1e-05)\n",
              "            (2): Hardswish()\n",
              "          )\n",
              "          (1): Conv2dNormActivation(\n",
              "            (0): Conv2d(184, 184, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=184, bias=False)\n",
              "            (1): FrozenBatchNorm2d(184, eps=1e-05)\n",
              "            (2): Hardswish()\n",
              "          )\n",
              "          (2): Conv2dNormActivation(\n",
              "            (0): Conv2d(184, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "            (1): FrozenBatchNorm2d(80, eps=1e-05)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (10): InvertedResidual(\n",
              "        (block): Sequential(\n",
              "          (0): Conv2dNormActivation(\n",
              "            (0): Conv2d(80, 184, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "            (1): FrozenBatchNorm2d(184, eps=1e-05)\n",
              "            (2): Hardswish()\n",
              "          )\n",
              "          (1): Conv2dNormActivation(\n",
              "            (0): Conv2d(184, 184, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=184, bias=False)\n",
              "            (1): FrozenBatchNorm2d(184, eps=1e-05)\n",
              "            (2): Hardswish()\n",
              "          )\n",
              "          (2): Conv2dNormActivation(\n",
              "            (0): Conv2d(184, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "            (1): FrozenBatchNorm2d(80, eps=1e-05)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (11): InvertedResidual(\n",
              "        (block): Sequential(\n",
              "          (0): Conv2dNormActivation(\n",
              "            (0): Conv2d(80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "            (1): FrozenBatchNorm2d(480, eps=1e-05)\n",
              "            (2): Hardswish()\n",
              "          )\n",
              "          (1): Conv2dNormActivation(\n",
              "            (0): Conv2d(480, 480, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=480, bias=False)\n",
              "            (1): FrozenBatchNorm2d(480, eps=1e-05)\n",
              "            (2): Hardswish()\n",
              "          )\n",
              "          (2): SqueezeExcitation(\n",
              "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
              "            (fc1): Conv2d(480, 120, kernel_size=(1, 1), stride=(1, 1))\n",
              "            (fc2): Conv2d(120, 480, kernel_size=(1, 1), stride=(1, 1))\n",
              "            (activation): ReLU()\n",
              "            (scale_activation): Hardsigmoid()\n",
              "          )\n",
              "          (3): Conv2dNormActivation(\n",
              "            (0): Conv2d(480, 112, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "            (1): FrozenBatchNorm2d(112, eps=1e-05)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (12): InvertedResidual(\n",
              "        (block): Sequential(\n",
              "          (0): Conv2dNormActivation(\n",
              "            (0): Conv2d(112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "            (1): FrozenBatchNorm2d(672, eps=1e-05)\n",
              "            (2): Hardswish()\n",
              "          )\n",
              "          (1): Conv2dNormActivation(\n",
              "            (0): Conv2d(672, 672, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=672, bias=False)\n",
              "            (1): FrozenBatchNorm2d(672, eps=1e-05)\n",
              "            (2): Hardswish()\n",
              "          )\n",
              "          (2): SqueezeExcitation(\n",
              "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
              "            (fc1): Conv2d(672, 168, kernel_size=(1, 1), stride=(1, 1))\n",
              "            (fc2): Conv2d(168, 672, kernel_size=(1, 1), stride=(1, 1))\n",
              "            (activation): ReLU()\n",
              "            (scale_activation): Hardsigmoid()\n",
              "          )\n",
              "          (3): Conv2dNormActivation(\n",
              "            (0): Conv2d(672, 112, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "            (1): FrozenBatchNorm2d(112, eps=1e-05)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (13): InvertedResidual(\n",
              "        (block): Sequential(\n",
              "          (0): Conv2dNormActivation(\n",
              "            (0): Conv2d(112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "            (1): FrozenBatchNorm2d(672, eps=1e-05)\n",
              "            (2): Hardswish()\n",
              "          )\n",
              "          (1): Conv2dNormActivation(\n",
              "            (0): Conv2d(672, 672, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=672, bias=False)\n",
              "            (1): FrozenBatchNorm2d(672, eps=1e-05)\n",
              "            (2): Hardswish()\n",
              "          )\n",
              "          (2): SqueezeExcitation(\n",
              "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
              "            (fc1): Conv2d(672, 168, kernel_size=(1, 1), stride=(1, 1))\n",
              "            (fc2): Conv2d(168, 672, kernel_size=(1, 1), stride=(1, 1))\n",
              "            (activation): ReLU()\n",
              "            (scale_activation): Hardsigmoid()\n",
              "          )\n",
              "          (3): Conv2dNormActivation(\n",
              "            (0): Conv2d(672, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "            (1): FrozenBatchNorm2d(160, eps=1e-05)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (14): InvertedResidual(\n",
              "        (block): Sequential(\n",
              "          (0): Conv2dNormActivation(\n",
              "            (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "            (1): FrozenBatchNorm2d(960, eps=1e-05)\n",
              "            (2): Hardswish()\n",
              "          )\n",
              "          (1): Conv2dNormActivation(\n",
              "            (0): Conv2d(960, 960, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=960, bias=False)\n",
              "            (1): FrozenBatchNorm2d(960, eps=1e-05)\n",
              "            (2): Hardswish()\n",
              "          )\n",
              "          (2): SqueezeExcitation(\n",
              "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
              "            (fc1): Conv2d(960, 240, kernel_size=(1, 1), stride=(1, 1))\n",
              "            (fc2): Conv2d(240, 960, kernel_size=(1, 1), stride=(1, 1))\n",
              "            (activation): ReLU()\n",
              "            (scale_activation): Hardsigmoid()\n",
              "          )\n",
              "          (3): Conv2dNormActivation(\n",
              "            (0): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "            (1): FrozenBatchNorm2d(160, eps=1e-05)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (15): InvertedResidual(\n",
              "        (block): Sequential(\n",
              "          (0): Conv2dNormActivation(\n",
              "            (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "            (1): FrozenBatchNorm2d(960, eps=1e-05)\n",
              "            (2): Hardswish()\n",
              "          )\n",
              "          (1): Conv2dNormActivation(\n",
              "            (0): Conv2d(960, 960, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=960, bias=False)\n",
              "            (1): FrozenBatchNorm2d(960, eps=1e-05)\n",
              "            (2): Hardswish()\n",
              "          )\n",
              "          (2): SqueezeExcitation(\n",
              "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
              "            (fc1): Conv2d(960, 240, kernel_size=(1, 1), stride=(1, 1))\n",
              "            (fc2): Conv2d(240, 960, kernel_size=(1, 1), stride=(1, 1))\n",
              "            (activation): ReLU()\n",
              "            (scale_activation): Hardsigmoid()\n",
              "          )\n",
              "          (3): Conv2dNormActivation(\n",
              "            (0): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "            (1): FrozenBatchNorm2d(160, eps=1e-05)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (16): Conv2dNormActivation(\n",
              "        (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (1): FrozenBatchNorm2d(960, eps=1e-05)\n",
              "        (2): Hardswish()\n",
              "      )\n",
              "    )\n",
              "    (fpn): FeaturePyramidNetwork(\n",
              "      (inner_blocks): ModuleList(\n",
              "        (0): Conv2dNormActivation(\n",
              "          (0): Conv2d(160, 256, kernel_size=(1, 1), stride=(1, 1))\n",
              "        )\n",
              "        (1): Conv2dNormActivation(\n",
              "          (0): Conv2d(960, 256, kernel_size=(1, 1), stride=(1, 1))\n",
              "        )\n",
              "      )\n",
              "      (layer_blocks): ModuleList(\n",
              "        (0-1): 2 x Conv2dNormActivation(\n",
              "          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        )\n",
              "      )\n",
              "      (extra_blocks): LastLevelMaxPool()\n",
              "    )\n",
              "  )\n",
              "  (rpn): RegionProposalNetwork(\n",
              "    (anchor_generator): AnchorGenerator()\n",
              "    (head): RPNHead(\n",
              "      (conv): Sequential(\n",
              "        (0): Conv2dNormActivation(\n",
              "          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "          (1): ReLU(inplace=True)\n",
              "        )\n",
              "      )\n",
              "      (cls_logits): Conv2d(256, 15, kernel_size=(1, 1), stride=(1, 1))\n",
              "      (bbox_pred): Conv2d(256, 60, kernel_size=(1, 1), stride=(1, 1))\n",
              "    )\n",
              "  )\n",
              "  (roi_heads): RoIHeads(\n",
              "    (box_roi_pool): MultiScaleRoIAlign(featmap_names=['0', '1', '2', '3'], output_size=(7, 7), sampling_ratio=2)\n",
              "    (box_head): TwoMLPHead(\n",
              "      (fc6): Linear(in_features=12544, out_features=1024, bias=True)\n",
              "      (fc7): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "    )\n",
              "    (box_predictor): FastRCNNPredictor(\n",
              "      (cls_score): Linear(in_features=1024, out_features=2, bias=True)\n",
              "      (bbox_pred): Linear(in_features=1024, out_features=8, bias=True)\n",
              "    )\n",
              "  )\n",
              ")"
            ]
          },
          "execution_count": 101,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model = fasterrcnn_mobilenet_v3_large_320_fpn(pretrained=False, num_classes=2)\n",
        "model.load_state_dict(torch.load('/content/faster_rcnn_mobilenetv2_50.pth'))\n",
        "model.eval()  # Przełącz model w tryb ewaluacji\n",
        "model.to(device)  # Upewnij się, że model jest na odpowiedniej urządzeniu"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FnS23MHLLb_S"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "\n",
        "# Załaduj obraz z dysku\n",
        "image_path = \"/content/data/images/source2/image_300.png\"  # Zamień 'your_image_name.jpg' na nazwę twojego obrazu\n",
        "image = cv2.imread(image_path)\n",
        "image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "from torchvision import utils\n",
        "import albumentations as A\n",
        "from albumentations.pytorch import ToTensorV2\n",
        "\n",
        "transform = A.Compose([A.Normalize(mean=[0, 0, 0], std=[255, 255, 255], max_pixel_value=1.0), ToTensorV2()])\n",
        "image_tensor = transform(image=image)['image']\n",
        "\n",
        "with torch.no_grad():\n",
        "    image_tensor = image_tensor.unsqueeze(0).to(device)  # Dodaj wymiar wsadu (batch) i przenieś tensor na urządzenie\n",
        "    prediction = model(image_tensor)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "6dzxuBdcPEr2"
      },
      "outputs": [],
      "source": [
        "# Filtruj bounding boxes, etykiety i wyniki na podstawie pewności powyżej 0.5\n",
        "boxes = prediction[0]['boxes'].cpu().numpy()\n",
        "scores = prediction[0]['scores'].cpu().numpy()\n",
        "\n",
        "filtered_indices = np.where(scores > 0.5)[0]\n",
        "filtered_boxes = boxes[filtered_indices]\n",
        "\n",
        "# Rysowanie bounding boxes na obrazie\n",
        "for box in filtered_boxes:\n",
        "    color = (0, 255, 0)  # Zielony\n",
        "    cv2.rectangle(image, (int(box[0]), int(box[1])), (int(box[2]), int(box[3])), color, 2)\n",
        "\n",
        "# Wyświetlenie obrazu z narysowanymi prostokątami\n",
        "plt.figure(figsize=(10, 10))\n",
        "plt.imshow(image)\n",
        "plt.axis('off')\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOzlmjq8WQFkiUlPi4ZCHMf",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}